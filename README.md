# Copilot Extension - AI-Powered Codebase Indexer

A VS Code extension that creates a **production-grade codebase index** for AI coding assistants. Uses the **3-Phase Architecture** employed by GitHub Copilot and similar tools.

## üéØ What Does This Extension Do?

This extension implements the **optimal architecture** for AI-assisted coding:

### **Phase 1: Static Index** üìä

Builds a lightweight context graph of your entire codebase **once** (or on file changes):

- File structure organized by directory
- Function/class signatures (no implementation code)
- Import relationships
- Call graph edges
- Symbol locations

**Output**: `quick_index.json` (5-50KB) + `search_index.json` (50-200KB)

### **Phase 2: Dynamic Search** üîç

When you ask a question, intelligently searches for relevant code:

- Extracts key terms from your query
- Searches the index for matching symbols
- Uses fast file system operations (like ripgrep)
- Fetches **only** the relevant code snippets

### **Phase 3: Context Assembly** üìù

Combines everything into perfect LLM context:

- Static index (codebase overview)
- Search results (relevant symbols)
- Actual code (from ripgrep)
- Your query

**Result**: Optimal context for AI with minimal token usage!

## üöÄ Features

- **Full Codebase Indexing**: Indexes all code files (20+ languages supported)
- **LSP-Based Analysis**: Uses VS Code's Language Server Protocol for 100% accurate parsing
- **Dual-Index System**:
  - **Quick Index**: Ultra-lightweight for LLM context
  - **Search Index**: Fast symbol lookup with locations
- **Smart Compression**: Skeleton mode (signatures only) keeps indices small
- **Security First**: Automatically redacts API keys, passwords, and secrets
- **Query System**: Natural language queries to find relevant code
- **Directory Organization**: Files grouped by directory for better readability
- **Summary Stats**: Language breakdown, top files, symbol counts

## üì¶ Installation

### From Source

1. Clone or download this repository
2. Open the folder in VS Code
3. Run `npm install` to install dependencies
4. Run `npm run compile` to build the extension
5. Press `F5` to open a new VS Code window with the extension loaded

### Package and Install

```bash
npm install -g @vscode/vsce
vsce package
code --install-extension copilot-extnsn-1.0.0.vsix
```

## üéÆ Usage

### Step 1: Generate Index (Phase 1)

1. Open any workspace in VS Code
2. Press **Cmd+Shift+P** (Mac) or **Ctrl+Shift+P** (Windows/Linux)
3. Type: `LogicGraph: Generate Index (Phase 1)`
4. Wait for indexing to complete

**Output Files**:

- `quick_index.json` - Lightweight codebase overview
- `search_index.json` - Detailed symbol locations

### Step 2: Query Codebase (Phase 2+3)

1. Press **Cmd+Shift+P**
2. Type: `LogicGraph: Query Codebase (Phase 2+3)`
3. Enter your question, e.g.:
   - "How does authentication work?"
   - "Show me the login function"
   - "Where is the database connection configured?"
4. View the assembled context (ready to send to ChatGPT/Claude!)

## üìÑ Output Structure

### `quick_index.json` Example

```json
{
  "_description": "Quick Index: Lightweight codebase overview for LLM context",
  "generated": "2026-01-04T10:30:00.000Z",
  "workspace": "/path/to/project",
  "summary": {
    "totalFiles": 150,
    "totalSymbols": 1243,
    "languages": {
      "typescript": 120,
      "javascript": 25,
      "python": 5
    }
  },
  "filesByDirectory": {
    "/src/app": [
      {
        "path": "/src/app/page.tsx",
        "language": "typescriptreact",
        "symbols": ["HomePage", "fetchData", "handleSubmit"]
      }
    ]
  }
}
```

### `search_index.json` Example

```json
{
  "_description": "Search Index: Detailed symbol locations for fast lookup",
  "summary": {
    "totalSymbols": 1243,
    "totalImports": 456,
    "topFiles": [{ "path": "/src/utils/helper.ts", "symbolCount": 45 }]
  },
  "symbolLocations": [
    {
      "symbol": "calculateTotal",
      "type": "Function",
      "file": "/src/utils/helper.ts",
      "line": 42,
      "signature": "function calculateTotal(items: Item[]): number"
    }
  ],
  "importMap": [
    {
      "file": "/src/app/page.tsx",
      "imports": [
        {
          "from": "/src/utils/helper.ts",
          "symbols": ["calculateTotal", "formatCurrency"]
        }
      ]
    }
  ]
}
```

## üîí Security

The extension automatically sanitizes sensitive information:

- Long API keys (20+ characters)
- Password/secret assignments
- Bearer tokens
- Long hexadecimal strings

Detected secrets are replaced with `[REDACTED]` in the output.

## ‚ö° Performance

- **Indexing**: ~100-500ms for most projects
- **Index Size**:
  - Quick Index: 5-50KB (tiny!)
  - Search Index: 50-200KB (still very small)
- **Query Time**: < 100ms to find relevant symbols
- **Strategy**: Signatures only (no full code) + on-demand retrieval

## üèóÔ∏è Architecture Details

### Supported Languages

TypeScript, JavaScript, Python, Java, C/C++, Go, Rust, C#, PHP, Ruby, Swift, Kotlin, Scala, Vue, Svelte

### Excluded Directories

`node_modules`, `dist`, `build`, `out`, `.git`, `venv`, `__pycache__`, `target`, `bin`, `obj`

### Optimization Strategies

1. **Skeleton Mode**: Stores only function signatures, not implementations
2. **File Size Limits**: Skips files > 100KB
3. **Directory Grouping**: Organizes files by directory for better navigation
4. **Smart Filtering**: Excludes build artifacts and dependencies
5. **Incremental Processing**: Shows progress every 10 files

## ü§ù How It Compares to GitHub Copilot

This extension uses the **same architecture** as professional AI coding tools:

| Feature             | This Extension | GitHub Copilot |
| ------------------- | -------------- | -------------- |
| Static Index        | ‚úÖ             | ‚úÖ             |
| Dynamic Search      | ‚úÖ             | ‚úÖ             |
| Context Assembly    | ‚úÖ             | ‚úÖ             |
| LSP Integration     | ‚úÖ             | ‚úÖ             |
| Skeleton Mode       | ‚úÖ             | ‚úÖ             |
| On-Demand Retrieval | ‚úÖ             | ‚úÖ             |

## üìù Use Cases

1. **AI-Assisted Coding**: Send assembled context to ChatGPT/Claude for code generation
2. **Codebase Understanding**: Quickly understand large/unfamiliar codebases
3. **Documentation**: Generate documentation from index structure
4. **Code Review**: Find all usages of a symbol across the codebase
5. **Refactoring**: Identify dependencies before making changes

## üõ†Ô∏è Development

### Build

```bash
npm run compile
```

### Watch Mode

```bash
npm run watch
```

### Project Structure

```
copilot-extnsn/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îî‚îÄ‚îÄ extension.ts       # Main extension logic
‚îú‚îÄ‚îÄ package.json           # Extension manifest
‚îú‚îÄ‚îÄ tsconfig.json          # TypeScript configuration
‚îî‚îÄ‚îÄ README.md              # This file
```

## üéì Educational Value

This extension demonstrates:

1. **Language Server Protocol (LSP)**: How modern IDEs understand code
2. **Static Analysis**: Analyzing code without running it
3. **Graph Algorithms**: Building call graphs and dependency trees
4. **Data Compression**: Reducing information while preserving meaning
5. **AI Context Management**: Optimal strategies for LLM integration
6. **VS Code Extension Development**: Building professional IDE tools

Perfect for learning about AI coding assistants, LSP, and static analysis!

## üìú License

MIT License - Feel free to use and modify as needed.

## ü§î FAQ

**Q: Why two JSON files?**  
A: `quick_index.json` is tiny and gives LLMs a complete overview. `search_index.json` enables fast symbol lookup for detailed queries.

**Q: Do I need to regenerate the index often?**  
A: Only when your code structure changes significantly (new files, renamed functions, etc.).

**Q: Can I use this with any LLM?**  
A: Yes! The assembled context works with ChatGPT, Claude, or any other LLM.

**Q: How is this different from RAG?**  
A: This is **deterministic** (uses LSP for accurate parsing) vs. RAG which is probabilistic (embeddings + vector search).

**Q: Does it work with large codebases?**  
A: Yes! The skeleton mode and smart filtering keep indices small even for 1000+ files.

---

**Built with ‚ù§Ô∏è for the AI coding community**
